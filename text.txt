\section{Our Approach}
In this section, we briefly elaborate on our proposed MPCANet architecture in Section~\ref{sec:arch}. The details of Multi-Physical Prior incorporating mechanism is illustrated in Section~\ref{sec:TPMA}. Second we describe the Thermo-RGB bi-attention block in Section~\ref{sec:tsm-cwi}. Then Section~\ref{sec:bsccd} introduce the Boundary-Semantic Synergistic Decoder. At last, we show the training loss in the Section~\ref{sec:MOCT}.
\subsection{Overview}
\label{sec:arch}
The overall architecture of our proposed MPCANet is demonstrated in Fig.~\ref{fig:pipeline}. 
Given an RGB image and a thermal image as input, we first employ dual-stream Swin Transformers to independently extract multi-scale features $\mathcal{X}_{r,ori} = \{X_{r,ori}^1, X_{r,ori}^2, X_{r,ori}^3, X_{r,ori}^4\}$ and $\mathcal{X}_{t,ori} = \{X_{t,ori}^1, X_{t,ori}^2, X_{t,ori}^3, X_{t,ori}^4\}$ from RGB and thermal modalities, respectively. Considering the thermal noise problem in RGB-T SOD, we first design a Multi-Physical Prior incorporation mechanism, which integrates physical priors into features at the source to mitigate the interference of thermal noise on feature extraction. Second, 





% Our proposed method is based on a dual-branch encoder-decoder architecture, with its overall design illustrated in (Fig.~\ref{fig:pipeline}). The architecture begins with a pair of parallel encoders that extract multi-scale feature pyramids from the RGB and TIR images, providing the foundational representations for subsequent cross-modal processing \cite{zhang2019rgb}\cite{zhou2023lsnet}.

% Building on this backbone, the feature stream is sequentially processed by our three core modules. First, features enter the Thermo-Physics Modulated Attention (TPMA, Sec.~\ref{sec:TPMA}) module. Its primary task is to perform a high-quality initial cross-modal fusion under the guidance of physical priors. To this end, it injects computed physical priors as an interpretable, structural bias directly into the attention logits, thereby suppressing thermal noise at the very source of feature interaction \cite{cong2022does}.

Next, the initially fused features are passed to the Thermo-Saliency Modulated Cross-Window Interaction (TSM-CWI, Sec.~\ref{sec:tsm-cwi}) module, which focuses on resolving cross-modal spatial misalignments. It achieves robust and flexible bidirectional feature alignment through a hybrid strategy that combines dynamic cross-window interaction with semantics-guided deformable sampling.

Finally, the aligned multi-scale features are fed into the Boundary-Semantic Coupled Cascaded Decoder (BS-CCD, Sec.~\ref{sec:bsccd}). It progressively recovers spatial resolution in a cascaded manner, placing a strong emphasis on enhancing the sharpness of object boundaries. Crucially, the decoder leverages a dedicated edge-aware unit at each stage to refine the structural information within skip-connections, preventing boundary degradation caused by modal inconsistencies. 

The entire network is trained end-to-end, coordinated by a Multi-Objective Consistent Training (MOCT, Sec.~\ref{sec:MOCT}) loss function that jointly supervises pixel-level saliency, region-level smoothness, and boundary-level structural consistency.

\subsection{Thermo-Physics Modulated Attention}
\label{sec:TPMA}
In cross-modal fusion, standard attention mechanisms dominate, but their purely data-driven nature poses risks: when inputs such as TIR images contain systematic biases (e.g., thermal noise), the attention weights inevitably inherit them. Feeding these contaminated features leads to suboptimal or even misleading fusion, creating an ``upstream pollution, downstream treatment'' dilemma—uncertainty arises at the very start of interaction, and later layers must waste capacity correcting it \cite{cong2022does}.

To address this fundamentally, we propose the Thermo-Physics Modulated Attention (TPMA). The key idea is to inject interpretable physical priors as inductive biases directly into attention weight generation, regularizing fusion at its source. This allows the network to proactively distinguish reliable signals from noise. TPMA consists of two parts: Physics-Prior Encoding and Physics-Guided Asymmetric Attention \cite{zhou2023position}.

\subsubsection{Physics-Prior Encoding}
The Physics-Prior Encoding module is designed to extract four complementary physical descriptors from the single-frame thermal image $T_{\text{input}} \in \mathbb{R}^{1 \times H \times W}$, characterizing the reliability of the thermal signal from different dimensions.

(a) Thermal Edge Prior ($E_{\text{thermal}}$): Based on the physical fact that true object boundaries often correspond to sharp first-order temperature changes and zero-crossings of the second-order response, we combine Sobel gradients with a multi-scale Laplacian-of-Gaussian (LoG) response to capture noise-robust edge information, which can be formulated as:
\begin{equation}
	\begin{split}
		E_{\mathrm{thermal}} = {} & w_{g}\,\mathrm{Norm}\!\big(G_{\mathrm{thermal}}\big) \\
		& + \sum_{\sigma \in \Sigma} w_{\sigma}\,\mathrm{Norm}\!\big(\lvert L_{\sigma}\rvert\big), \\
		& \quad w_{g}\ge 0,\; w_{\sigma}\ge 0,
	\end{split}
\end{equation}
where $E_{\mathrm{thermal}}$ is the final fused edge prior, $w_g$ and $w_{\sigma}$ are non-negative weights, $\mathrm{Norm}(\cdot)$ is a per-map normalization function, and $\Sigma$ is a set of scales. Among them, the thermal gradient magnitude map $G_{\mathrm{thermal}}$ is:
\begin{equation}
	G_{\mathrm{thermal}} = \sqrt{\left(\frac{\partial T_{\mathrm{input}}}{\partial x}\right)^{2} + \left(\frac{\partial T_{\mathrm{input}}}{\partial y}\right)^{2}},
\end{equation}
where $\frac{\partial}{\partial x}$ and $\frac{\partial}{\partial y}$ denote partial derivatives along the spatial dimensions. the Laplacian response at scale $\sigma$  ($L_{\sigma}$) can be:
\begin{equation}
	L_{\sigma} = \mathrm{LoG}_{\sigma} * T_{\mathrm{input}},
\end{equation}
where $*$ denotes convolution. This prior provides the network with a strong signal regarding boundary locations \cite{canny2009computational}.

(b) Thermal Diffusion Prior ($D_{\text{thermal}}$): According to the law of heat conduction, heat diffuses from high-temperature to low-temperature regions, forming a smooth temperature field. We approximate this process via Gaussian smoothing to generate the thermal diffusion prior.
\begin{equation}
	D_{\text{thermal}}=G_{\sigma}^{(\tau)}*T_{\text{input}},
\end{equation}
where $D_{\text{thermal}}$ is the thermal diffusion prior, and $G_{\sigma}^{(\tau)}$ is a Gaussian smoothing kernel applied $\tau$ times at scale $\sigma$.
It helps to suppress isolated noisy hotspots and enhances the physical consistency within salient regions \cite{perona2002scale}.

(c) Pseudo-Time Thermal Inertia Prior ($I_{\text{thermal}}$): Objects with high thermal inertia exhibit slow temperature changes. To simulate this dynamic property from a single frame, we construct a pseudo-time diffusion sequence and compute the sum of differences between adjacent ``time steps''.
\begin{equation}
	T^{(s+1)} = G_{\sigma} * T^{(s)},
\end{equation}
\begin{equation}
	I_{\text{thermal}} = \sum_{s=0}^{S-1} \beta_s | T^{(s+1)} - T^{(s)} |,
\end{equation}
where $T^{(s)}$ is the diffused thermal image at pseudo-time step $s$, with $T^{(0)} = T_{\text{input}}$. $I_{\text{thermal}}$ is the thermal inertia prior, and $\beta_s$ is a weighting factor, and $S$ is the total number of pseudo-time steps.
This prior helps the network identify core salient regions that are thermally stable.

(d) Pseudo-Band Emissivity Difference Prior ($A_{\text{thermal}}$): Different materials have different thermal emissivities, a difference that can be reflected in the frequency domain. We employ a bank of log-Gabor filters $\{H_b\}$ to decompose the image in the frequency domain.
\begin{equation}
	A_{\text{thermal}} = \Big[\, \mathcal{F}^{-1}\!\big(H_1 \cdot \mathcal{F}(T_{\text{input}})\big),\ \ldots,\ \mathcal{F}^{-1}\!\big(H_B \cdot \mathcal{F}(T_{\text{input}})\big) \Big],
\end{equation}
where $A_{\text{thermal}}$ is the concatenated responses of the filter bank, $\mathcal{F}(\cdot)$ and $\mathcal{F}^{-1}(\cdot)$ are the forward and inverse Fourier Transforms, and $\{H_b\}_{b=1}^{B}$ is a bank of log-Gabor filters.
This captures material-related fine-grained textures, providing discriminative information that purely spatial operators cannot \cite{fischer2007self}.

Finally, we concatenate these four prior maps and use a $1 \times 1$ convolution $\phi_{1\times 1}$ for adaptive fusion, generating a unified physics-prior tensor $P_{\text{thermal}}$.
\begin{equation}
	P_{\text{thermal}} = \phi_{1\times 1}(\mathrm{Concat}[E_{\text{thermal}}, D_{\text{thermal}}, I_{\text{thermal}}, A_{\text{thermal}}]),
\end{equation}
where $\mathrm{Concat}[\cdot]$ denotes concatenation along the channel dimension, and $\phi_{1\times 1}$ is a $1 \times 1$ convolutional layer. To facilitate reuse across scales, we downsample $P_{\text{thermal}}$ to each scale $l$ using average pooling, denoted as $P_{\text{thermal}}^{(l)} \in \mathbb{R}^{H_l \times W_l \times C_p}$.

\subsubsection{Physics-Guided Asymmetric Attention}
After obtaining the physics-prior tensor $P_{\text{thermal}}$, we design an asymmetric attention modulation strategy that aims to precisely apply this physical knowledge to the TIR branch while avoiding unnecessary interference with the high-quality RGB branch.

The strategy first modulates the attention weights via ``Logit Injection.'' Specifically, the query ($Q_t$), key ($K_t$), and value ($V_t$) are all obtained by applying linear projections on the TIR encoder features $E_t^{(l)}$. After computing the similarity between $Q_t$ and $K_t$, we add the physics prior $P_{\text{thermal}}$ directly to the attention logits. This operation fundamentally alters the attention distribution before the Softmax normalization, thereby guiding the network to focus on physically reliable regions. This is calculated as follows:
\begin{equation}
    \tilde{A}^{(l)}_t = \mathrm{Softmax}\!\left(\frac{Q_t K_t^\top}{\sqrt{d_k}} + \lambda\,\Pi^{(l)}\!\big(P^{(l)}_{\text{thermal}}\big)\right).
\end{equation}
where $\tilde{A}^{(l)}_t$ is the physics-biased attention map, $d_k$ is the dimension of the key features, $\Pi(\cdot)$ is a projection function to match resolutions, i.e., within each window at level $l$, $\Pi(P_{\text{thermal}}^{(l)})$ applies a $1{\times}1$ conv and spatial pooling to produce a key-wise scalar bias $\mathbf{b}^{(l)}\!\in\!\mathbb{R}^{N_k}$ (one scalar per key position), which is broadcast across queries and heads to align with $Q_tK_t^\top$. $\lambda$ is a learnable scaling parameter \cite{liu2021swinnet}.

Concurrently, to enhance the representation of salient regions, we apply semantic gating to the Value features, $V_t^{(l)}$. Instead of directly relying on decoder predictions at this stage, we employ a high-level semantic map $S_{\text{sem}}$ derived from the RGB encoder’s deep features (or their probabilistic form), which provides reliable saliency priors. This map is spatially aligned to the current scale and used for multiplicative gating of the values, formulated as:
\begin{equation}
	\tilde{V}_t^{(l)} = V_t^{(l)} \odot \big( 1 + \gamma_{\text{sem}}\, S_{\text{sem}} \big),
\end{equation}
where $\tilde{V}_t^{(l)}$ is the semantically gated value feature, $\gamma_{\text{sem}}$ is a learnable scaling parameter, and $\odot$ denotes element-wise multiplication \cite{tang2022hrtransnet}. 

Finally, we combine the physics-biased attention map with the semantically gated value features to obtain the stage-wise TIR-to-RGB output,
\begin{equation}
F_{\text{T}\to\text{R}}^{(l)}=\tilde{A}_t^{(l)} \cdot \tilde{V}_t^{(l)} .
\end{equation}
Notably, for the reverse RGB-to-TIR path we do not apply this physics-prior bias, thereby preserving the original RGB information and constituting the asymmetric modulation. 
We compute
\begin{equation}
A_r^{(l)}=\mathrm{Softmax}\!\left(\frac{Q_t^{(l)}\,K_r^{(l)\top}}{\sqrt{d_k}}\right),\qquad 
F_{\text{R}\to\text{T}}^{(l)} = A_r^{(l)}\,V_r^{(l)} .
\end{equation}

Both outputs are then “written back’’ to their original branches via residual fusion to form the \emph{updated features at stage $l$}:
\begin{equation}
\begin{aligned}
X_{r,\mathrm{upd}}^{(l)} &= X_{r}^{(l)} + \phi_{1\times1}\!\Big(\,[X_{r}^{(l)},\, F_{\text{T}\to\text{R}}^{(l)}]\,\Big),\\
X_{t,\mathrm{upd}}^{(l)} &= X_{t}^{(l)} + \phi_{1\times1}\!\Big(\,[X_{t}^{(l)},\, F_{\text{R}\to\text{T}}^{(l)}]\,\Big).
\end{aligned}
\end{equation}

These updated features are \emph{consumed by the remaining sub-layers within stage $l$}  before leaving stage $l$, with $l\in\{1,2,3,4\}$.



Through the TPMA mechanism, we provide the subsequent processing pipeline with a set of high-quality cross-modal features that have been "purified" and enhanced by physical information. However, these features may still be spatially misaligned. In the next section, we will introduce how the TSM-CWI module builds upon this foundation to address the challenge of precise cross-modal feature alignment \cite{zhang2019rgb}.



\subsection{Thermo-Saliency Modulated Cross-Window Interaction}
\label{sec:tsm-cwi}
Although the TPMA module provides ``physically-purified'' features, it does not address the inherent spatial misalignment in RGB-T data. Existing windowed attention methods are often limited by their \textbf{static window sizes}, which struggle to adapt to varying object scales, and their \textbf{rigid interaction}, which cannot handle non-rigid deformations. This often results in a trade-off between losing details and inaccurate alignment \cite{liu2021swin}\cite{liu2021swinnet}.

To overcome these challenges, we propose the \textbf{Thermo-Saliency Modulated Cross-Window Interaction (TSM-CWI)} mechanism. Its core idea is to \textbf{leverage high-level semantic information (i.e., saliency) as a dual guide to simultaneously drive the attention's ``perceptual scope'' (window size) and its ``alignment precision'' (sampling locations)}. This design enables our model to achieve a dynamic balance between global consistency and local detail fidelity, and is realized through two synergistic sub-modules: saliency-aware dynamic windowing and semantics-guided deformable alignment \cite{tang2022hrtransnet}\cite{zhou2023position}.

\subsubsection{Saliency-Aware Dynamic Windowing}
At level $l$, given the RGB and TIR features $X_r^l, X_t^l \in \mathbb{R}^{H_l \times W_l \times C_l}$ from TPMA, we perform bidirectional cross-window interaction. This process is guided by the physics prior $P_{\text{thermal}}^{(l)}$ and an encoder-side semantic map $S^{l}$, which is generated by a lightweight head at each encoder level to avoid causal dependencies \cite{zhou2023lsnet}.

To allow the window size to adapt to object scales, we first dynamically select the window size based on the statistical properties of the high-level saliency map $p$ (the probabilistic form of $S^{(l)}$).
\begin{equation}
	\bar{s}_\omega = \frac{1}{|\omega|} \sum_{u \in \omega} p(u), \quad \bar{s} = \frac{1}{N_w} \sum_{\omega} \bar{s}_\omega,
\end{equation}
where $\bar{s}_\omega$ is the average saliency probability within a window $\omega$, $N_w$ is the total number of windows, and $\bar{s}$ is the average saliency across the entire feature map.
\begin{equation}
	w^\star = 
	\begin{cases}
		w_{\text{small}}, & \text{if } \bar{s} \ge \tau, \\
		w_{\text{large}}, & \text{otherwise},
	\end{cases}
\end{equation}
where $w^\star$ is the dynamically selected window size. When salient objects in the scene are concentrated (higher $\bar{s}$), we use small windows to focus on details; otherwise, we use large windows to capture broader context. In our implementation, we adopt a ``per-window'' strategy to achieve more fine-grained adaptivity: each window $\omega$ independently selects its size $w(\omega) \in \{w_{\text{small}}, w_{\text{large}}\}$ based on its own average saliency $\bar{s}_{\omega}$. This approach, which preserves the global binary selection as a special case, better handles mixed scenes containing both dense and sparse salient regions \cite{liu2021swin}. 

After selecting $w^\star$ dynamically, the window size $w^\star$ is used in the subsequent cross-window interaction (CWI) to adjust the receptive field of each window, allowing the model to focus on detailed areas with small windows or capture global context with large windows.

\subsubsection{Semantics-Guided Deformable Alignment}
	After determining the adaptive window size $w^\star$, we perform cross-window attention between RGB and TIR features. This step inherits the physics bias and semantic gating from TPMA to enhance robustness. The attention weights are defined as:
\begin{equation}
	\mathrm{Attn}(Q,K) = \mathrm{Softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}} + \lambda\,\Pi^{(l)}\!\big(P^{(l)}_{\text{thermal}}\big) + M\right),
\end{equation}
where $Q$ and $K$ are the query and searched features, $d_k$ is the feature dimension, $\lambda$ controls the physics prior, $\Pi(\cdot)$ projects the prior to the target resolution, and $M$ is a shifted-window mask (set to zero for regular partitioning). This formulation ensures that the attention distribution integrates both physics priors and structural constraints.  

At high-resolution levels (e.g., $l \in \{3,4\}$), precise alignment is further achieved via an \textbf{Offset Head}. Guided by the saliency map $p^{(l)}$, the offset head takes $[X_r^l, X_t^l, p^{(l)}]$ as input and predicts offsets $\Delta^{(l)} \in \mathbb{R}^{H_l \times W_l \times 2K}$. These offsets deformably sample the searched stream’s Value features $V$, producing
\textbf{Semantic gating of Value.} With the encoder-side saliency prior $S_{\text{sem}}^{(l)} := p^{(l)}$, we gate the Value:
\begin{equation}
  \bar V = V \odot \bigl(1 + \gamma_{\rm sem}\, S_{\text{sem}}^{(l)}\bigr).
\end{equation}
\textbf{Deformable sampling.} The gated values are then deformably sampled:
\begin{equation}
  \tilde V = \mathcal{S}(\bar V;\Delta^{(l)}).
\end{equation}


The deformably aligned $\tilde V$ replaces $V$ in the aggregation, and the attended output within each window is obtained:
\begin{equation}
    O = \mathrm{Attn}(Q,K)\,\tilde V.
\end{equation}

Here, $\mathrm{Attn}(Q,K)$ provides the physics-biased attention weights, while $\tilde V$ provides the deformably aligned values. Their product $O$ represents the final attended output for each window, encoding both semantic adaptivity and spatial alignment \cite{zhou2023position}.  

The window-level outputs $O$ are then aggregated across all windows to form the bidirectional cross-modal representations:
\begin{equation}
	Z_r^l = \mathrm{CWI}(X_r^l,X_t^l,w^\star) = \mathrm{Agg}(O_{r \leftarrow t}), 
\end{equation}
\begin{equation}
	Z_t^l = \mathrm{CWI}(X_t^l,X_r^l,w^\star) = \mathrm{Agg}(O_{t \leftarrow r}),
\end{equation}
where $\mathrm{Agg}(\cdot)$ reassembles window-level outputs back to the full feature map by reversing the window partition, aggregating heads, and applying a $1{\times}1$ linear projection for shape recovery. Using Eqs.~(18)–(19), we obtain the bidirectional cross-modal representations: $Z_r^{(l)}=\mathrm{Agg}(O_{T\rightarrow R}^{(l)})$, which aligns thermal structures to the RGB view (RGB queries TIR), and $Z_t^{(l)}=\mathrm{Agg}(O_{R\rightarrow T}^{(l)})$, which transfers RGB semantics to the TIR view (TIR queries RGB). This bidirectional design enforces geometric consistency and preserves complementary semantics in both modalities, mitigating the bias of one-sided alignment \cite{tu2021multi}.


Finally, the two aligned outputs are concatenated and fused through convolution:
\begin{equation}
	\mathbf{F}_l = \mathrm{Conv}\!\left([Z_r^l,Z_t^l]\right).
\end{equation}

In summary, the attention mechanism first produces physics-biased weights $\mathrm{Attn}(Q,K)$, the offset head adaptively aligns the Value features to obtain $\tilde V$, and their aggregation yields the window-level outputs $O$. These outputs are then combined bidirectionally into $Z_r^l$ and $Z_t^l$, which are fused into $\mathbf{F}_l$, providing spatially aligned and semantically coherent features for decoding \cite{tu2022weakly}.



%\subsection{Boundary-Semantic Coupled Cascaded Decoder}
%\label{sec:bsccd}
%In RGB-T decoding, directly fusing skip-connections from the encoder is problematic due to the inherent misalignment between RGB texture boundaries and TIR thermal boundaries. This ``modal conflict'' leads to blurry or inaccurate object edges and weakens the effect of preceding fusion modules. To address this issue, we propose the \textbf{Boundary-Semantic Coupled Cascaded Decoder (BS-CCD)}, which separates decoding into two complementary tasks: \textbf{purifying structural information via boundary prediction} and \textbf{using high-level semantics to guide feature recovery}. 
%
%Instead of raw skip-connections, we design an edge branch whose prediction serves as a spatial filter, while semantic maps from deeper stages constrain the focus on salient regions. At each stage $l$, the decoder processes the fused features $\mathbf{F}_l$ from TSM-CWI and the skip-features $\mathbf{X}_r^l$ from the RGB encoder.
%
%\subsubsection{Edge-Aware Skip Fusion}
%To mitigate modal conflict, we first predict an edge map $\hat{\mathbf{E}}$ from the deepest RGB feature $\mathbf{X}_r^4$ and propagate it across stages. At stage $l$, the input is
%\begin{equation}
%	\mathbf{U}_l = \big[\,\mathrm{Up}_l(\mathbf{F}_{l}),\ \mathbf{X}_r^l\,\big]\in\mathbb{R}^{H_l\times W_l\times (C_F+C_E)},
%\end{equation}
%which is fed into a lightweight convolutional head:
%\begin{equation}
%	\hat{\mathbf{E}}^{(l)}=\mathrm{EdgeHead}(\mathbf{U}_l)=\sigma\!\Big(\phi_{1\times1}\big(\phi_{3\times3}^{(2)}(\mathbf{U}_l)\big)\Big)\in[0,1]^{H_l\times W_l},
%\end{equation}
%where $\phi_{3\times3}^{(2)}$ denotes two stacked $3{\times}3$ convs with BN+ReLU, $\phi_{1\times1}$ reduces channels, and $\sigma$ is Sigmoid. To enrich details at higher resolutions, dilation $d\in\{2,3\}$ can be used in the first conv of the top stages. The head is shared across stages for parameter efficiency. The predicted edge response is converted into attention and applied to filter the concatenated features:
%\begin{equation}
%	\mathbf{A}_{\text{edge}}^{(l)}=\sigma(\mathrm{Conv}_{1\times1}(\hat{\mathbf{E}}^{(l)})),\quad
%	\tilde{\mathbf{F}}_l=\phi(\mathbf{U}_l)\odot \mathbf{A}_{\text{edge}}^{(l)}.
%\end{equation}
%For supervision, soft boundary targets $\tilde{\mathbf{B}}^\star$ are derived from saliency ground-truth $\mathbf{S}^\star$ using LoG zero-crossings (or Canny), and $\mathrm{BCE}(\hat{\mathbf{E}}^{(l)},\tilde{\mathbf{B}}^\star)$ is added as boundary loss $\mathcal{L}_{\text{bd}}$. During inference, only $\mathbf{A}_{\text{edge}}^{(l)}$ is used for filtering.
%
%\subsubsection{Semantic Gating for Channel-Spatial Reweighting}
%After boundary purification, we use semantic guidance to emphasize salient structures. At stage $l$, the guidance map $p$ is the upsampled saliency prediction $\hat{\mathbf{S}}_{l+1}$ from stage $(l{+}1)$, or the encoder prior $p^{(L)}$ when $l=L$. We construct a semantic gate:
%\begin{equation}
%	\mathbf{A}_{\text{sem}}=1+\gamma_{\text{dec}}\cdot \mathrm{Norm}(p),
%\end{equation}
%where $\gamma_{\text{dec}}$ controls the gating strength. The reweighted feature is then used to produce the saliency prediction:
%\begin{equation}
%	\hat{\mathbf{S}}_l=\psi\!\big(\tilde{\mathbf{F}}_l\odot \mathbf{A}_{\text{sem}}\big),
%\end{equation}
%with $\psi(\cdot)$ a $1\times1$ conv for channel reduction. This simple gating method prevents the model from enhancing details in non-salient areas too much.
%
%In summary, BS-CCD purifies encoder skip-connections through boundary attention and enhances salient structures through semantic gating. This design alleviates cross-modal boundary conflicts and yields high-quality, multi-scale saliency predictions.


\subsection{Boundary-Semantic Coupled Cascaded Decoder}
\label{sec:bsccd}
In RGB-T decoding, directly using skip-connections often causes ``modal conflict,'' since RGB texture boundaries and TIR thermal boundaries are naturally inconsistent. Such misalignment results in blurred or inaccurate boundaries, undermining the effectiveness of previous fusion and alignment modules. To address this issue, we propose the \textbf{Boundary-Semantic Coupled Cascaded Decoder (BS-CCD)}, which separates decoding into two synergistic steps: Edge-Aware Skip Fusion and Semantic Gating \cite{zhou2023lsnet}\cite{liu2021swinnet}.

At each decoding stage $l$, the decoder receives both the fused features $\mathbf{F}_l$ from the TSM-CWI module and the skip features $\mathbf{X}_r^l$ from the RGB encoder \cite{tu2021multi}.

\begin{figure}[!t]	\centering{\includegraphics[width=1\linewidth]{pipeline_2.pdf}}
	\vspace{-1.2em}
	\caption{Architecture of the Boundary-Semantic Coupled Cascaded Decoder (BS-CCD), consisting of Edge-Aware Skip Fusion (top) and Semantic Gating (bottom).}
	\label{fig:pipeline2}
	\vspace{-1em}
\end{figure}

\subsubsection{Edge-Aware Skip Fusion}
To eliminate modal conflict, we introduce an \emph{edge branch} that predicts structural boundaries and uses them to purify skip-features.  
At stage $l$, the input is the concatenation of the upsampled fused features and the skip:
\begin{equation}
	\mathbf{U}_l = [\,\mathrm{Up}(\mathbf{F}_l),\ \mathbf{X}_r^l\,] \in \mathbb{R}^{H_l \times W_l \times (C_F + C_E)}.
\end{equation}

The edge branch, denoted as $\mathrm{EdgeHead}$, is a lightweight convolutional head consisting of two stacked $3\times3$ conv layers (each followed by BatchNorm and ReLU), a $1\times1$ conv for channel compression, and a Sigmoid activation:
\begin{equation}
	\hat{\mathbf{E}}^{(l)} = \mathrm{EdgeHead}(\mathbf{U}_l) = \sigma\!\Big(\phi_{1\times1}(\phi^{(2)}_{3\times3}(\mathbf{U}_l))\Big) \in [0,1]^{H_l \times W_l}.
\end{equation}
Here, $\phi^{(2)}_{3\times3}$ indicates two stacked $3\times3$ conv blocks, $\phi_{1\times1}$ reduces channels to $1$, and $\sigma$ is the Sigmoid. For high-resolution stages, dilated convolutions with rates $d \in \{2,3\}$ may be applied to enlarge the receptive field. To ensure parameter efficiency and cross-stage consistency, the same $\mathrm{EdgeHead}$ is shared across all stages, i.e., $\mathrm{EdgeHead}^{(l)} \equiv \mathrm{EdgeHead}$ \cite{zhou2018unet++}.

The predicted edge map is transformed into attention and applied to the features:
\begin{equation}
	\mathbf{A}_{\text{edge}}^{(l)} = \sigma(\phi_{1\times1}(\hat{\mathbf{E}}^{(l)})), \quad 
	\tilde{\mathbf{F}}_l = \phi(\mathbf{U}_l) \odot \mathbf{A}_{\text{edge}}^{(l)},
\end{equation}
where $\phi$ is a $3\times3$ conv used for feature refinement.  
For supervision, a soft boundary target $\tilde{\mathbf{B}}^\star$ is derived from the saliency ground truth $\mathbf{S}^\star$ using LoG zero-crossings (or Canny). A boundary loss $\mathcal{L}_{\text{bd}} = \mathrm{BCE}(\hat{\mathbf{E}}^{(l)}, \tilde{\mathbf{B}}^\star)$ is applied at all stages. During inference, only $\mathbf{A}_{\text{edge}}^{(l)}$ is used, while explicit edge maps are discarded \cite{canny2009computational}\cite{arbelaez2010contour}.

\subsubsection{Semantic Gating}
After boundary purification, we further apply semantic guidance to highlight salient regions. At stage $l$, the guidance map $p$ is the upsampled saliency prediction $\hat{\mathbf{S}}_{l+1}$ from the deeper stage $(l{+}1)$; for the top stage ($l=L$), we use the encoder-side prior $p^{(L)}$. A semantic gate is defined as:
\begin{equation}
	\mathbf{A}_{\text{sem}} = 1 + \gamma_{\text{dec}} \cdot \mathrm{Norm}(p),
\end{equation}
where $\gamma_{\text{dec}}$ is learnable and $\mathrm{Norm}(\cdot)$ normalizes the saliency map. The gate reweights the purified features:
\begin{equation}
	\hat{\mathbf{S}}_l = \psi(\tilde{\mathbf{F}}_l \odot \mathbf{A}_{\text{sem}}),
\end{equation}
where $\psi$ is a $1\times1$ conv generating the saliency prediction at stage $l$ \cite{wang2020deep}.

In summary, BS-CCD purifies encoder skip-connections through boundary attention and enhances salient structures through semantic gating. This design alleviates cross-modal boundary conflicts and yields high-quality predictions \cite{zhou2023lsnet}\cite{tu2021multi}.



\subsection{Training}
\label{sec:MOCT}

\begin{figure}[!t]	\centering{\includegraphics[width=1\linewidth]{pipeline_3.pdf}}
	\vspace{-1.2em}
	\caption{Illustration of our Multi-Objective Consistent Training (MOCT, Sce.~\ref{sec:MOCT}) strategy, designed to resolve the ``internal discoordination'' common in complex networks. Instead of a single objective, MOCT applies a composite loss function that provides supervisory signals across multiple dimensions: (i) primary pixel/region saliency, (ii) structure-preserving smoothness, (iii) explicit boundary alignment, and (iv) cross-modal alignment consistency. This holistic approach guides the entire framework to learn as a cohesive whole.}
	\label{fig:motivation}
	\vspace{-1em}
\end{figure}

Training a complex, multi-module network with only a simple loss function (e.g., BCE + Dice) often leads to ``internal discoordination''. For instance, an internal edge prediction may become inconsistent with the boundaries of the final saliency map, or a feature alignment module might excessively distort features to optimize the final metric. This ``black-box'' approach to training ignores critical intermediate processes, leading to unstable training and limited generalization \cite{zhou2021ecffnet}.

To address this, we propose the \textbf{Multi-Objective Consistent Training (MOCT)} strategy. Its core idea is to \textbf{apply end-to-end constraints on the network from multiple dimensions, such as pixel, region, structure, boundary, and feature, through a composite loss function}. This strategy not only supervises the final output but also applies supervisory signals to key intermediate representations (like the predicted edge map) and inter-module relationships (like the consistency of aligned features). This guides the entire framework to learn as a cohesive whole, ensuring both high internal consistency and high quality in the final output \cite{zhou2023position}.

Our overall loss function, $\mathcal{L}$, is a weighted sum of five key loss terms:
\begin{equation} 
	\begin{split} 
		\mathcal{L} = {} & \lambda_{\text{bce}}\mathcal{L}_{\text{BCE}} + \lambda_{\text{dice}}\mathcal{L}_{\text{Dice}} + \lambda_{\text{sm}}\mathcal{L}_{\text{Smooth}} \\ 
		& + \lambda_{\text{edge}}\mathcal{L}_{\text{Edge}} + \lambda_{\text{cons}}\mathcal{L}_{\text{Cons}} 
	\end{split} 
\end{equation}
where the $\lambda$ terms are weights that balance the different loss components. In our experiments, these are set empirically to $\lambda_{\text{bce}}=0.5$, $\lambda_{\text{dice}}=0.5$, $\lambda_{\text{sm}}=0.8$, $\lambda_{\text{edge}}=0.5$, and $\lambda_{\text{cons}}=0.1$.

\begin{table*}[t]
	\centering
	\caption{SOTA Comparison of E-Measure ($E$), S-Measure ($S$), and F-Measure ($F$) trained on UVT20K, evaluated on UVT20K and UVT2000. The best result in each row is marked in bold, and the top three results across all methods are highlighted with \textbf{\textcolor{red}{red}}, \textbf{\textcolor{ForestGreen}{green}}, and \textbf{\textcolor{blue}{blue}}.
	}
	\label{tab:sota_uvt20k_train}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lcccccccccccccccc}
			\toprule
			\multicolumn{2}{c}{\textbf{Method}} & %
			MIDD\cite{tu2021multi} & CSRNet\cite{huo2021efficient} & CGFNet\cite{wang2021cgfnet} & SwinNet\cite{liu2021swinnet} & OSRNet\cite{huo2022real} &
			TNet\cite{cong2022does} & DCNet\cite{zhu2025dc} & HRTrans\cite{tang2022hrtransnet} & MCFNet\cite{ma2023modal} & LSNet\cite{zhou2023lsnet} &
			CAVER\cite{pang2023caver} & LAFB\cite{wang2024learning} & SACNet\cite{wang2024alignment} & PCNet\cite{wang2025alignment} & Ours\\
			\midrule
			\multirow{3}{*}{\textbf{UVT20K}}
			& $E\uparrow$ & 0.842 & 0.790 & 0.819 & 0.850 & 0.835 & \textbf{\textcolor{blue}{0.876}} & 0.853 & 0.839 & 0.875 & 0.828 & \textbf{\textcolor{blue}{0.876}} & 0.858 & 0.819 & \textbf{\textcolor{ForestGreen}{0.897}} & \textbf{\textcolor{red}{0.919}} \\
			& $S\uparrow$ & 0.836 & 0.753 & 0.829 & 0.841 & 0.807 & 0.856 & 0.808 & 0.852 & 0.842 & 0.834 & \textbf{\textcolor{blue}{0.861}} & 0.847 & 0.829 & \textbf{\textcolor{ForestGreen}{0.872}} & \textbf{\textcolor{red}{0.881}}\\
			& $F\uparrow$ & 0.743 & 0.635 & 0.709 & 0.728 & 0.730 & 0.783 & 0.776 & 0.710 & \textbf{\textcolor{blue}{0.800}} & 0.699 & 0.790 & 0.757 & 0.709 & \textbf{\textcolor{ForestGreen}{0.822}} & \textbf{\textcolor{red}{0.849}}\\
			\midrule
			\multirow{3}{*}{\textbf{UVT2000}}
			& $E\uparrow$ & 0.727 & 0.658 & 0.704 & 0.780 & 0.764 & 0.782 & \textbf{\textcolor{blue}{0.808}} & 0.706 & 0.784 & 0.711 & 0.782 & 0.774 & 0.792 & \textbf{\textcolor{ForestGreen}{0.851}} & \textbf{\textcolor{red}{0.874}}\\
			& $S\uparrow$ & 0.778 & 0.655 & 0.764 & 0.790 & 0.741 & \textbf{\textcolor{blue}{0.792}} & 0.767 & 0.758 & 0.774 & 0.763 & 0.786 & 0.778 & \textbf{\textcolor{ForestGreen}{0.795}} & \textbf{\textcolor{red}{0.819}} & \textbf{\textcolor{blue}{0.792}} \\
			& $F\uparrow$ & 0.563 & 0.420 & 0.539 & 0.592 & 0.567 & 0.610 & \textbf{\textcolor{blue}{0.632}} & 0.525 & 0.621 & 0.527 & 0.616 & 0.594 & 0.601 & \textbf{\textcolor{ForestGreen}{0.686}} & \textbf{\textcolor{red}{0.699}}\\
			\bottomrule
		\end{tabular}
	}
	\vspace{-1em}
\end{table*}

\subsubsection{Primary Saliency Supervision}
This is the fundamental supervision for the saliency detection task. We use the Binary Cross-Entropy (BCE) loss and the Dice loss in parallel. The BCE loss supervises the predicted probability at the pixel level, while the Dice loss measures the overlap between the predicted mask and the ground-truth label at the region level \cite{tu2021multi}.
\begin{equation}
	\mathcal{L}_{\text{BCE}}=-\frac{1}{N}\sum_{i}\big[y_i\log(\hat{s}_i)+(1-y_i)\log(1-\hat{s}_i)\big],
\end{equation}
where $y_i$ and $\hat{s}_i$ are the ground-truth label and the predicted saliency probability for pixel $i$, respectively.
\begin{equation}
	\mathcal{L}_{\text{Dice}}
	=1-\frac{2\sum_i \hat{s}_i y_i+\epsilon}{\sum_i \hat{s}_i+\sum_i y_i+\epsilon},
\end{equation}
where $\epsilon$ is a smoothing term to prevent division by zero. Their combination ensures both pixel-wise accuracy and region-level integrity.

\subsubsection{Structure-Preserving Smoothness Loss}
To encourage smoothness in the prediction while preserving edges, we introduce a smoothness loss guided by the gradient of the ground-truth label $\mathbf{Y}$ \cite{perona2002scale}\cite{charbonnier1997deterministic}.
\begin{equation}
	\mathcal{L}_{\text{Smooth}} = \mathbb{E}\big[\rho(e^{-\alpha|\nabla_x \mathbf{Y}|}|\nabla_x \hat{\mathbf{S}}|) + \rho(e^{-\alpha|\nabla_y \mathbf{Y}|}|\nabla_y \hat{\mathbf{S}}|)\big],
\end{equation}
where $\hat{\mathbf{S}}$ is the final saliency prediction, $\nabla$ is the gradient operator, and $\rho(z)=\sqrt{z^2+\varepsilon^2}$ is the Charbonnier penalty function. This loss applies strong smoothing in flat regions and weak smoothing near edges, thus suppressing noise while keeping boundaries sharp.

\subsubsection{Explicit Edge Alignment Loss}
To ensure that the edge prediction $\hat{\mathbf{E}}$ from the decoder's edge branch is consistent with the boundary derived from the final saliency map $\mathcal{E}(\hat{\mathbf{S}})$, we constrain both of them towards the ground-truth boundary $\mathbf{E}^*=\mathcal{E}(\mathbf{Y})$.
\begin{equation}
	\mathcal{L}_{\text{Edge}}
	=\|\mathcal{E}(\hat{\mathbf{S}})-\mathbf{E}^*\|_1
	+\beta\,\|\hat{\mathbf{E}}-\mathbf{E}^*\|_1,
\end{equation}
where $\mathcal{E}(\cdot)$ is an edge extraction operator (e.g., Sobel), and $\beta$ is a balancing weight. This loss forces the two internal boundary representations to align with each other, leading to more stable final contours \cite{canny2009computational}\cite{arbelaez2010contour}.

\subsubsection{Cross-Modal Alignment Consistency Loss}
To supervise the alignment quality of the TSM-CWI module, we enforce that the aligned features should remain consistent with the original features of the target modality. For the RGB-to-TIR alignment, for example:
\begin{equation}
	\mathcal{L}_{\text{Cons}}
	=\sum_{l\in L}
	\|\mathrm{Norm}(\mathbf{F}^{r\rightarrow t}_l)-\mathrm{Norm}(\mathbf{T}^{\text{orig}}_l)\|_1,
\end{equation}
where $\mathbf{F}^{r\rightarrow t}_l$ are the features from RGB aligned to TIR at layer $l$, $\mathbf{T}^{\text{orig}}_l$ are the original TIR features, and $\mathrm{Norm}(\cdot)$ is a normalization function. This loss applies supervision directly in the feature domain, avoiding complex geometric reprojections and making the alignment training more stable \cite{zhou2021ecffnet}\cite{zhou2023position}.
